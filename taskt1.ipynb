{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "default_dtype = torch.float32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_sort(x_input):\n",
    "    \"\"\"\n",
    "    Sorts each row so the maximum panel length is always first.\n",
    "    This resolves cyclic symmetry.\n",
    "    \"\"\"\n",
    "    x = x_input.copy()\n",
    "    for i, x_row in enumerate(x):\n",
    "        max_index = np.argmax(x_row)\n",
    "        x[i] = np.roll(x_row, shift=-max_index)\n",
    "    return x\n",
    "\n",
    "def mirror_sort(x_input):\n",
    "    \"\"\"\n",
    "    Ensures the largest neighboring panel is in a consistent position.\n",
    "    This resolves mirror symmetry.\n",
    "    \"\"\"\n",
    "    x = x_input.copy()\n",
    "    for i, x_row in enumerate(x):\n",
    "        right_panel = x_row[1]\n",
    "        left_panel = x_row[-1]\n",
    "        if left_panel > right_panel:\n",
    "            x[i, 1:] = np.flip(x_row[1:])\n",
    "    return x\n",
    "\n",
    "def compute_aspect_ratios(X):\n",
    "    \"\"\"\n",
    "    Normalize panel lengths by the longest panel.\n",
    "    This prevents a single large panel from dominating the model's learning.\n",
    "    \"\"\"\n",
    "    aspect_ratios = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        aspect_ratios[i] = X[i] / np.max(X[i]) \n",
    "    return aspect_ratios\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def disambiguate_symmetries(x_input):\n",
    "    \"\"\"\n",
    "    Applies both cyclic and mirror sorting to fully resolve symmetries.\n",
    "    \"\"\"\n",
    "    x = np.sort(x_input, axis=1)[:, ::-1].copy()\n",
    "    x = mirror_sort(x)\n",
    "    x = compute_aspect_ratios(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14250, 9) Validation shape: (750, 9)\n"
     ]
    }
   ],
   "source": [
    "def load_fence_data(file_paths, input_dim=9):\n",
    "\n",
    "    dfs = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        panel_cols = [str(i) for i in range(input_dim)]\n",
    "        \n",
    "        for col in panel_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0.0\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    X = data[panel_cols].values.astype(np.float32)\n",
    "    X = disambiguate_symmetries(X)  # Apply symmetry resolution\n",
    "\n",
    "    y = data['CE'].values.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "train_files = [\n",
    "    \"data/kaggle_train_5_fences.csv\",\n",
    "    \"data/kaggle_train_7_fences.csv\",\n",
    "    \"data/kaggle_train_9_fences.csv\"\n",
    "]\n",
    "X_train, y_train = load_fence_data(train_files)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Train shape:\", X_train.shape, \"Validation shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset wrapping processed panel length data.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, dtype=torch.float32):\n",
    "        self.X = torch.tensor(X, dtype=dtype)\n",
    "        self.y = torch.tensor(y, dtype=dtype).unsqueeze(1)  # Reshape to (N,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = FenceDataset(X_train, y_train)\n",
    "val_dataset = FenceDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=9):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1, bias=False),\n",
    "            nn.Sigmoid()  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 0.1643, Accuracy: 94.26%\n",
      "Epoch 2/100 - Loss: 0.0547, Accuracy: 97.55%\n",
      "Epoch 3/100 - Loss: 0.0394, Accuracy: 98.40%\n",
      "Epoch 4/100 - Loss: 0.0415, Accuracy: 98.18%\n",
      "Epoch 5/100 - Loss: 0.0388, Accuracy: 98.37%\n",
      "Epoch 6/100 - Loss: 0.0332, Accuracy: 98.55%\n",
      "Epoch 7/100 - Loss: 0.0423, Accuracy: 98.19%\n",
      "Epoch 8/100 - Loss: 0.0321, Accuracy: 98.62%\n",
      "Epoch 9/100 - Loss: 0.0364, Accuracy: 98.53%\n",
      "Epoch 10/100 - Loss: 0.0425, Accuracy: 98.29%\n",
      "Epoch 11/100 - Loss: 0.0329, Accuracy: 98.55%\n",
      "Epoch 12/100 - Loss: 0.0325, Accuracy: 98.62%\n",
      "Epoch 13/100 - Loss: 0.0355, Accuracy: 98.59%\n",
      "Epoch 14/100 - Loss: 0.0272, Accuracy: 98.84%\n",
      "Epoch 15/100 - Loss: 0.0254, Accuracy: 98.93%\n",
      "Epoch 16/100 - Loss: 0.0306, Accuracy: 98.70%\n",
      "Epoch 17/100 - Loss: 0.0346, Accuracy: 98.51%\n",
      "Epoch 18/100 - Loss: 0.0313, Accuracy: 98.68%\n",
      "Epoch 19/100 - Loss: 0.0248, Accuracy: 98.96%\n",
      "Epoch 20/100 - Loss: 0.0284, Accuracy: 98.81%\n",
      "Epoch 21/100 - Loss: 0.0302, Accuracy: 98.64%\n",
      "Epoch 22/100 - Loss: 0.0279, Accuracy: 98.88%\n",
      "Epoch 23/100 - Loss: 0.0284, Accuracy: 98.70%\n",
      "Epoch 24/100 - Loss: 0.0288, Accuracy: 98.73%\n",
      "Epoch 25/100 - Loss: 0.0294, Accuracy: 98.66%\n",
      "Epoch 26/100 - Loss: 0.0288, Accuracy: 98.73%\n",
      "Epoch 27/100 - Loss: 0.0285, Accuracy: 98.81%\n",
      "Epoch 28/100 - Loss: 0.0252, Accuracy: 99.00%\n",
      "Epoch 29/100 - Loss: 0.0287, Accuracy: 98.88%\n",
      "Epoch 30/100 - Loss: 0.0284, Accuracy: 98.83%\n",
      "Epoch 31/100 - Loss: 0.0252, Accuracy: 98.91%\n",
      "Epoch 32/100 - Loss: 0.0341, Accuracy: 98.58%\n",
      "Epoch 33/100 - Loss: 0.0243, Accuracy: 99.01%\n",
      "Epoch 34/100 - Loss: 0.0268, Accuracy: 98.88%\n",
      "Epoch 35/100 - Loss: 0.0244, Accuracy: 99.03%\n",
      "Epoch 36/100 - Loss: 0.0251, Accuracy: 98.84%\n",
      "Epoch 37/100 - Loss: 0.0268, Accuracy: 98.89%\n",
      "Epoch 38/100 - Loss: 0.0227, Accuracy: 99.03%\n",
      "Epoch 39/100 - Loss: 0.0259, Accuracy: 98.89%\n",
      "Epoch 40/100 - Loss: 0.0239, Accuracy: 98.91%\n",
      "Epoch 41/100 - Loss: 0.0247, Accuracy: 98.97%\n",
      "Epoch 42/100 - Loss: 0.0282, Accuracy: 98.83%\n",
      "Epoch 43/100 - Loss: 0.0251, Accuracy: 98.91%\n",
      "Epoch 44/100 - Loss: 0.0216, Accuracy: 99.05%\n",
      "Epoch 45/100 - Loss: 0.0258, Accuracy: 98.98%\n",
      "Epoch 46/100 - Loss: 0.0233, Accuracy: 98.95%\n",
      "Epoch 47/100 - Loss: 0.0238, Accuracy: 98.96%\n",
      "Epoch 48/100 - Loss: 0.0218, Accuracy: 99.07%\n",
      "Epoch 49/100 - Loss: 0.0237, Accuracy: 98.94%\n",
      "Epoch 50/100 - Loss: 0.0274, Accuracy: 98.76%\n",
      "Epoch 51/100 - Loss: 0.0258, Accuracy: 99.01%\n",
      "Epoch 52/100 - Loss: 0.0217, Accuracy: 98.96%\n",
      "Epoch 53/100 - Loss: 0.0223, Accuracy: 99.09%\n",
      "Epoch 54/100 - Loss: 0.0234, Accuracy: 99.03%\n",
      "Epoch 55/100 - Loss: 0.0247, Accuracy: 98.99%\n",
      "Epoch 56/100 - Loss: 0.0251, Accuracy: 98.88%\n",
      "Epoch 57/100 - Loss: 0.0240, Accuracy: 98.99%\n",
      "Epoch 58/100 - Loss: 0.0220, Accuracy: 99.05%\n",
      "Epoch 59/100 - Loss: 0.0228, Accuracy: 99.05%\n",
      "Epoch 60/100 - Loss: 0.0225, Accuracy: 99.09%\n",
      "Epoch 61/100 - Loss: 0.0198, Accuracy: 99.06%\n",
      "Epoch 62/100 - Loss: 0.0239, Accuracy: 99.02%\n",
      "Epoch 63/100 - Loss: 0.0234, Accuracy: 99.08%\n",
      "Epoch 64/100 - Loss: 0.0212, Accuracy: 99.07%\n",
      "Epoch 65/100 - Loss: 0.0234, Accuracy: 99.09%\n",
      "Epoch 66/100 - Loss: 0.0296, Accuracy: 98.84%\n",
      "Epoch 67/100 - Loss: 0.0240, Accuracy: 99.04%\n",
      "Epoch 68/100 - Loss: 0.0245, Accuracy: 99.05%\n",
      "Epoch 69/100 - Loss: 0.0222, Accuracy: 99.02%\n",
      "Epoch 70/100 - Loss: 0.0237, Accuracy: 99.05%\n",
      "Epoch 71/100 - Loss: 0.0200, Accuracy: 99.21%\n",
      "Epoch 72/100 - Loss: 0.0209, Accuracy: 99.08%\n",
      "Epoch 73/100 - Loss: 0.0204, Accuracy: 99.21%\n",
      "Epoch 74/100 - Loss: 0.0237, Accuracy: 98.98%\n",
      "Epoch 75/100 - Loss: 0.0216, Accuracy: 99.00%\n",
      "Epoch 76/100 - Loss: 0.0258, Accuracy: 98.95%\n",
      "Epoch 77/100 - Loss: 0.0195, Accuracy: 99.11%\n",
      "Epoch 78/100 - Loss: 0.0192, Accuracy: 99.25%\n",
      "Epoch 79/100 - Loss: 0.0236, Accuracy: 98.95%\n",
      "Epoch 80/100 - Loss: 0.0257, Accuracy: 99.02%\n",
      "Epoch 81/100 - Loss: 0.0194, Accuracy: 99.30%\n",
      "Epoch 82/100 - Loss: 0.0204, Accuracy: 99.12%\n",
      "Epoch 83/100 - Loss: 0.0203, Accuracy: 99.14%\n",
      "Epoch 84/100 - Loss: 0.0224, Accuracy: 99.03%\n",
      "Epoch 85/100 - Loss: 0.0198, Accuracy: 99.12%\n",
      "Epoch 86/100 - Loss: 0.0194, Accuracy: 99.18%\n",
      "Epoch 87/100 - Loss: 0.0200, Accuracy: 99.09%\n",
      "Epoch 88/100 - Loss: 0.0209, Accuracy: 99.09%\n",
      "Epoch 89/100 - Loss: 0.0221, Accuracy: 99.00%\n",
      "Epoch 90/100 - Loss: 0.0224, Accuracy: 99.05%\n",
      "Epoch 91/100 - Loss: 0.0206, Accuracy: 99.23%\n",
      "Epoch 92/100 - Loss: 0.0186, Accuracy: 99.24%\n",
      "Epoch 93/100 - Loss: 0.0225, Accuracy: 99.07%\n",
      "Epoch 94/100 - Loss: 0.0217, Accuracy: 99.02%\n",
      "Epoch 95/100 - Loss: 0.0202, Accuracy: 99.15%\n",
      "Epoch 96/100 - Loss: 0.0185, Accuracy: 99.19%\n",
      "Epoch 97/100 - Loss: 0.0211, Accuracy: 99.08%\n",
      "Epoch 98/100 - Loss: 0.0191, Accuracy: 99.18%\n",
      "Epoch 99/100 - Loss: 0.0214, Accuracy: 99.10%\n",
      "Epoch 100/100 - Loss: 0.0205, Accuracy: 99.12%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, lr):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for panels_batch, labels_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(panels_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * panels_batch.size(0)\n",
    "            correct += ((outputs >= 0.5).float() == labels_batch).sum().item()\n",
    "            total += labels_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "model = NeuralNetwork(input_dim=9).to(torch.device(\"cpu\"))\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=100, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 misclassified samples.\n",
      "Example 1:\n",
      "  Panels: [1.         0.4164022  0.2788603  0.21470524 0.19842508 0.17381363\n",
      " 0.17157032 0.09322661 0.00333265]\n",
      "  True Label: 1.0, Predicted Label: 0.0\n",
      "Example 2:\n",
      "  Panels: [1.         0.94227237 0.18488751 0.11356357 0.00462162 0.\n",
      " 0.         0.         0.        ]\n",
      "  True Label: 0.0, Predicted Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "def find_misclassified_examples(model, dataloader):\n",
    "    \"\"\"\n",
    "    Identifies misclassified samples from the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for panels_batch, labels_batch in dataloader:\n",
    "            outputs = model(panels_batch)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            incorrect_indices = (preds != labels_batch).squeeze().nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            for i in incorrect_indices:\n",
    "                errors.append((panels_batch[i].cpu().numpy(), labels_batch[i].item(), preds[i].item()))\n",
    "    \n",
    "    return errors\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "misclassified_samples = find_misclassified_examples(model, val_loader)\n",
    "\n",
    "print(f\"Found {len(misclassified_samples)} misclassified samples.\")\n",
    "for i, (panels, true_label, pred_label) in enumerate(misclassified_samples[:-1]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Panels: {panels}\")\n",
    "    print(f\"  True Label: {true_label}, Predicted Label: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: torch.Size([45000, 9])\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(file_path, input_dim=9):\n",
    "    df = pd.read_csv(file_path)\n",
    "    panel_cols = [str(i) for i in range(input_dim)]\n",
    "    # Replace NaN values with 0 in the panel columns\n",
    "    df[panel_cols] = df[panel_cols].fillna(0)\n",
    "    \n",
    "    X = df[panel_cols].values.astype(np.float32)\n",
    "    X = disambiguate_symmetries(X)\n",
    "    test_ids = df['id'].values\n",
    "    return torch.tensor(X), test_ids\n",
    "\n",
    "# Example usage:\n",
    "X_test, test_ids = load_test_data(\"data/kaggle_hidden_test_fences.csv\", input_dim=9)\n",
    "print(\"Test data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submissiont1.csv'\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_test = model(X_test)\n",
    "    preds_test = (outputs_test >= 0.5).float().cpu().numpy().flatten()\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'prediction': preds_test.astype(int)})\n",
    "submission_df.to_csv(\"submissiont1.csv\", index=False)\n",
    "print(\"Submission file saved as 'submissiont1.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
